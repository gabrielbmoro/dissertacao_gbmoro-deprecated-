# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Measuring Hardware Counters  for HPC Application Phase Detection
#+AUTHOR: Gabriel Bronzatti Moro, Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)  Gabriel(G) Lucas(L)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference,letter,10pt,final]
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lipsum}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \newcommand{\review}[1]{\textcolor[rgb]{1,0,0}{[Lucas: #1]}}

# You need Org 8.3.5 and Emacs 24 to make this work.
# If you do, just type make (thanks Luka Stanisic for this).

* Gráficos NAS                                                     :noexport:
** Plot da FT
*** L2

#+begin_src R :results output graphics :file "img/ftBNas_Analise.pdf" :exports both :session *RFib* 

library(dplyr);

df <- read.csv("../../dados/exp1_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L2 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/ftBNas_Analise.pdf]]

**** 30ms

#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp1_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:       Time Metric Socket  N     mean          se
: 1 9.832468     M7      1 16 31.00176 0.002447148
:        Time Metric Socket  N     mean          se
: 1 0.3410059     M7      1 16 6.786985 0.005029964

**** 100ms  
#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp3_NASandLikwid/ftl2.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
      Time Metric Socket  N     mean          se
1 9.615836     M7      1 16 30.81585 0.002073066
       Time Metric Socket  N     mean         se
1 0.3575629     M7      2 16 10.01222 0.02316528
#+end_example

*** L3
#+begin_src R :results output graphics :file "img/ftBNas_Analise_l3.pdf" :exports both :session *RFib* 

library(dplyr);

df <- read.csv("../../dados/exp2_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L3 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/ftBNas_Analise_l3.pdf]]

#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp2_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:        Time Metric Socket  N     mean        se
: 1 0.2776482     M7      1 16 37.61564 0.2987426
:       Time Metric Socket  N       mean           se
: 1 8.711887     M7      1 16 0.02094844 5.839419e-05

** Plot da LU
*** L2

#+begin_src R :results output graphics :file "img/luBNas_Analise.pdf" :exports both :session *RFib* 

library(dplyr);

df <- read.csv("../../dados/exp1_NASandLikwid/luB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L2 Cache Misses (%)");

#+end_src

#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp1_NASandLikwid/luB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:       Time Metric Socket  N     mean         se
: 1 33.42106     M7      2 16 27.99985 0.04944031
:        Time Metric Socket  N     mean         se
: 1 0.1006167     M7      1 16 10.88676 0.02663008

*** L3

#+begin_src R :results output graphics :file "img/luBNas_Analise_l3.pdf" :exports both :session *RFib* 

library(dplyr);

df <- read.csv("../../dados/exp2_NASandLikwid/luB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L3 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/luBNas_Analise_l3.pdf]]


#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp2_NASandLikwid/luB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:        Time Metric Socket  N     mean        se
: 1 0.1005844     M7      1 16 13.77685 0.1511483
:       Time Metric Socket  N       mean           se
: 1 36.26222     M7      2 16 0.07087374 0.0005140726

** Plot da CG
*** L2

#+begin_src R :results output graphics :file "img/cgBNas_Analise.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp1_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L2 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/cgBNas_Analise.pdf]]

#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp1_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:       Time Metric Socket  N    mean         se
: 1 23.69983     M7      2 16 38.6508 0.02485503
:         Time Metric Socket  N     mean        se
: 1 0.05055852     M7      1 16 10.21882 0.0773729


*** L3
#+begin_src R :results output graphics :file "img/cgBNas_Analise_l3.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp2_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L3 Cache Misses (%)");

#+end_src


#+RESULTS:
[[file:img/cgBNas_Analise_l3.pdf]]

#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp2_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:         Time Metric Socket  N     mean        se
: 1 0.05055831     M7      2 16 23.65833 0.2532902
:       Time Metric Socket  N        mean           se
: 1 21.36921     M7      2 16 0.004947738 1.722305e-05

** Plot do SP
#+begin_src R :results output graphics :file "img/spBNas_Analise.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp1_NASandLikwid/spB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L2 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/spBNas_Analise.pdf]]

#+begin_src R :results output graphics :file "img/spBNas_Analise_l3.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp2_NASandLikwid/spB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L3 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/spBNas_Analise_l3.pdf]]

** Plot do UA

#+begin_src R :results output graphics :file "img/uaBNas_Analise.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp1_NASandLikwid/uaB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L2 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/uaBNas_Analise.pdf]]

#+begin_src R :results output graphics :file "img/uaBNas_Analise_l3.pdf" :exports both :session *RF* 

library(dplyr);

df <- read.csv("../../dados/exp2_NASandLikwid/uaB.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();
k <- 	arrange(k,as.integer(k$Core));
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
middle <- mean(k$Value);
k$Socket <- ifelse(k$Core %% 2 == 0,1,2);
g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();
library(ggplot2);
ggplot(g[g$Metric == "M7",], aes(x=Time, y=mean,color=as.factor(Socket))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,100) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="CPU Socket") +
      labs(x = "Runtime (seconds)", y= "Average L3 Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/uaBNas_Analise_l3.pdf]]

* Conversas e definições sobre o artigo                            :noexport:
** Proposta de Estrutura para o Artigo                              :Gabriel:
- Professor, acho interessante a seguinte estrutura para escrevermos
  nosso artigo:

#+BEGIN_EXAMPLE
1. Introduction 


2. Related Works PRAZO - ATÉ Sexta-feira 05/08
     - Utilizar os trabalhos: Laurenzano e Freeh 
     - Procurar mais alguns a apartir de um mapeamento sistemático da literatura

3. Methodology
     - Penso aqui em apresentar as características do DoE realizado para executar o experimento (PRAZO - ATÉ Segunda-feira 08/08)

4. Preliminary Results PRAZO - ATÉ Terça-feira 09/08
      - Penso aqui em usar o benchmark Rodinia executando duas aplicações, uma chamada BFS (representando uma aplicação memory-bound) e a Back Propagation (representando uma aplicação cpu-bound)

5. Conclusion PRAZO - ATÉ Terça-feira 09/08
      p1: comentar resultados

    5.1 Future Work
#+END_EXAMPLE

** Por que BFS e Back Propagation como benchmarks?                   :Lucas:

Estávamos usando a orion3 para realizar os experimentos relacionados a
energia, pois a turing não tem suporte RAPL para isso. Mas como tu por
enquanto não está medindo isso, apenas os contadores, acho que tudo
bem. É importante ter consciência que os contadores disponíveis em uma
máquina com suporte de medição de energia podem potencialmente ser
diferentes dos contadores disponíveis na turing. Estou curioso para
ver as primeiras medições. Todas as medidas devem ser registradas em
arquivos CSV no próprio repositório (quando o tamanho é adequado para
git - arquivos de mais de 10 mega começam a ser questionáveis). 

Teus deadlines me parecem adequados, mas o ideal é que o processo
fosse iterativo. O ideal seria terminar tudo até essa sexta 5/ago para
permitir bons refinamentos. Avisa-me quando estiver com algo passível
de leitura. 

*** Resposta:                                                     :Gabriel:
Olá professor, perfeitamente, o senhor sabe que estávamos pensando em
quais contadores usar, nisso avaliando a fundo o artigo do *Laurenzano
et al.*, foi possível encontrar que no experimento ele utilizou
contadores para estimar a taxa de hit dos diferentes níveis de cache,
outro contador para contabilizar a quantidade de operações de
ponto-flutuante realizadas e a quantidade de operações FP realizadas
sobre inteiro. A partir disso, eu investiguei os contadores
disponibilizados pelo PAPI, e dentre eles, para identificar o que
queremos, podemos usar os seguintes: *PAPI_L1_DCA* (acessos à L1),
*PAPI_L2_DCA* (acessos à L2), *PAPI_L3_DCA* (acessos à L3), *PAPI_L1_DCH*
(taxa de hits da L1), *PAPI_L2_DCH* (taxa de hits da L2) e *PAPI_L3_DCA*
(número de misses na L3). Vale lembrar, que ainda tenho que verificar
a disponibilidade desses contadores na =turing=, a mesma está bloqueada:

#+begin_src sh :results output :exports both
gbmoro@portal:~$ ssh -X gabrielbmoro@turing
gabrielbmoro@turing's password: 
Welcome to Ubuntu 12.04.5 LTS (GNU/Linux 3.13.0-48-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Thu Aug  4 00:19:56 BRT 2016

  System load:    0.05              Processes:             602
  Usage of /home: 31.0% of 4.51TB   Users logged in:       1
  Memory usage:   2%                IP address for eth0:   143.54.12.105
  Swap usage:     0%                IP address for virbr0: 192.168.122.1

  Graph this data and manage this system at:
    https://landscape.canonical.com/

166 packages can be updated.
112 updates are security updates.

New release '14.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Your Hardware Enablement Stack (HWE) is supported until April 2017.

Please DO NOT install packages or create users without talking to the admins.

Last login: Wed Aug  3 23:08:54 2016 from portal.inf.ufrgs.br
locked by user 'vemabaunza' at Wed Aug  3 18:43:52 BRT 2016
-m Victor Martinez - sera liberada 4/08/2016 de manha
Connection to turing closed.

#+end_src

- Quanto aos traces gerados, esses estão na turing, e não os commitei
  para o git por causa do tamanho. Vou fazer uma execução na =turing=
  usando o minibench o que o senhor acha?
  Esse minibench tem mini-aplicações (ideia sugerida pelo Matthias),
  as quais são rápidas de executar, permitindo que o experimento seja
  executado mais rapidamente e que eu possa já na sexta-feira ter um
  volume de trabalho significativo (primeira versão do artigo). Nunca
  trabalhei com o minibench, mas acho uma boa ideia. 

Mensionei o BFS, porque aplicações que utilizam grafos, tendem a ser
memory-bound, pois o índice de cache miss nessas aplicações é muito
alto, visto que o grafo não é armazenado de maneira contínua na
memória é via referência, o processo de busca envolve vários
acessos à memória, podendo gerar vários misses. Depois pensei na Back
Propagation, porque comparado ao BFS, ela é uma aplicação mais
CPU-bound, o que seria interessante analisar nas diferentes fases o
comportamento dessas duas aplicações paralelas. Mas depois, o Matthias
me falou do MiniBench, o que achei interessante e que pode nos ajudar,
o que o senhor acha?

* IEEETran configuration for org export + ignore tag (Start Here)  :noexport:

#+begin_src emacs-lisp :results output :session :exports both
(add-to-list 'load-path ".")
(require 'ox-extra)
(ox-extras-activate '(ignore-headlines))
(add-to-list 'org-latex-classes
             '("IEEEtran"
               "\\documentclass{IEEEtran}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

#+RESULTS:

* *The Paper*                                                       :ignore:
** Latex configurations                                             :ignore:
** Frontpage                                                        :ignore:
#+BEGIN_LaTeX
\title{Measuring Hardware Counters for \\ HPC Application Phase Detection}

\author{
\IEEEauthorblockN{Gabriel Bronzatti Moro, Lucas Mello Schnorr \\}
\IEEEauthorblockA{Institute of Informatics, Federal University of Rio Grande do Sul \\
Caixa Postal 15064 –- CEP 91501-970 Porto Alegre -- RS -- Brazil\\
Email: \textit{\{gabriel.bmoro,schnorr\}@inf.ufrgs.br}\\
}
}
#+END_LaTeX

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+LaTeX: \begin{abstract}

Besides reducing the execution time of parallel applications, the
power consumption is an increasingly addressed problem in
High-Performance Computing. A parallel program can be divided into
regions, which can have particular characteristics,
for instance, a behavior more CPU or memory bound. This paper presents
a preliminary effort to measure performance counters along time to
enable the automatic detection of memory-bound regions. Once
attained, the automatic detection differs from the previous works
since it does not require any code instrumentation, making it less 
intrusive. Three NAS Parallel Benchmark (NPB) applications are used in the
experiments: the Discrete 3D fast Fourier Transform (FT), the
Lower-Upper Gauss-Seidel solver (LU), and the Conjugate Gradient
(CG). As hardware counters, we measure the L2 and L3 cache miss rate
along time using the likwid's timeline mode, a tool to measure
hardware counters from the user space. The experiments enable us to
identify possible memory-bound code regions by correlating with
changes in cache miss rates. We intend to configure a suitable
processor frequency for each memory-bound parallel region of the
application, reducing the energy consumption of the application with
minimal performance loss.

#+LaTeX: \end{abstract}

** Introduction

#+LaTeX: %- Large HPC applications are usually composed by many parallel regions
  #+LaTeX: %- Give some examples
#+LaTeX: %- Each code region has its own memory/cpu/io resource requirements
  #+LaTeX: %- Some might be more memory-bound, others cpu-bound, for example

Large HPC applications are composed of many parallel regions that are
executed by different threads. For example, in an application that
simulates the heat transfer through of a metal plate, we could define two
parallel regions: the first to define the initial condition of the plate
and another part would be responsible for calculating the heat
transfer across different points of the plate for a number of timesteps.
Each parallel region has its own characteristic. Some may be
considered memory-bound, with a high rate of cache misses,
while others may be considered more CPU-bound, with a high instruction
execution rate, and even others might be considered IO-bound.
In the previous example of the
heat plate, one could measure the hardware counters at a given
frequency to define the main characteristic of each parallel region.
An automatic detection of memory-bound parallel regions enable one to
adjust the processor frequency, possibly reducing energy consumption
with minor performance penalties in execution time.

#+LaTeX: %- Automatically detecting such regions could potentially lead to
  #+LaTex: % per-parallel region improvements such as energy and performance
  #+LaTeX: % improvements by adopting an appropriate processor frequency to
  #+LaTeX: % execute

#+LaTeX: %- The idea of this work is to measure hardware counters along time in
#+LaTeX: %  order to correlate their values against the different code region
#+LaTeX: %  - With this information, we intend to detect memory-bound code
#+LaTeX: %    regions that could be potential candidates for energy reduction
#+LaTeX: %    strategies (mainly DVFS)
#+LaTeX: %  - Once the memory-bound code regions have been detected, we intend
#+LaTeX: %    to apply Design of Experiments techniques to find the best
#+LaTeX: %    processor frequency configuration for each region, pretty similar
#+LaTeX: %    to what has been done already lfgmillani2016reppar, but
#+LaTeX: %    automatically.

The main objective of this work is to measure hardware counters at
every given time interval to discover memory-bound code regions.  Once
these regions have been detected, we intend to apply Design of
Experiments screening techniques \cite{jain1991art} to find the best
processor frequency configuration for each region, pretty similar to
what has been done already \cite{millani2016fr}, but
automatically. This paper presents our preliminary results by showing
a time-oriented method to collect hardware counters, using likwid's
timeline mode \cite{treibig2010likwid}. The preliminary results
indicate that the collected data can possibly enable the automated
identification of memory-bound code regions.

#+LaTeX: %- Paper structure

The paper is organized as follows. Section \ref{sec:relatedwork}
presents related work regarding automatic phase detection for HPC
applications. In Section \ref{sec:methodology} is presented our
proposal and its corresponding methodology. The Section
\ref{sec:results} describes the platform used in the experiments and
the preliminary results. Section \ref{sec:conclusion} concludes the
paper listing the main contributions and future works.

*** Previous structure (in portuguese)                           :noexport:

- contextualizar o problema, relacionando o trabalho já feito pelo
  Luís Felipe, o porque pensar numa detecção automatizada da troca de
  fase entre as threads, o que o trabalho poderá somar ao projeto
  existente.

- apresentar o objetivo do trabalho, o qual será apresentado como um
  "estudo de viabilidade" do trabalho, mostrando que é possível
  realizá-lo técnicamente e que esse é um dos passos fundamentais para
  colocá-lo em prática

- análisar os resultados preliminares

- apresentar a organização do artigo

_Revisão Lucas_

- Cuidar a escrita em português, veja o acento nestas palavras
  - tecnicamente
  - analisar
- 

** Related Work
\label{sec:relatedwork}

#+LaTeX: %- There is no definitive solution to detect if a code region is more
#+LaTeX:  %memory or CPU bound.
#+LaTeX:  %- Usually hard. counters are globally aggregated
#+LaTeX:  %- Automatic techniques usually rely on specific hardware counters

There is no definitive solution to detect if a code region is more
memory or CPU bound. Some works focus more on phase detection to
sequential applications
\cite{spiliopoulos2012power}\cite{laurenzano2011reducing}. Spiliopoulos
et al. \cite{spiliopoulos2012power} present in his work a tool that
analyzes the behavior of a sequential application by detailed analysis
of its execution phases, based on cache misses of the different
levels of cache. The identified phase may be comprised of a set of
program functions, which are grouped by having a similar behavior. The
identification of the behavior of these functions is performed in
accordance with a prior history of execution, based on the trace
generated by the application. The tool generated identifies the best processor frequency to
be used in each phase to best performance and reduce energy
consumption. This paper analyzes only sequential applications, in this
perspective the identification of the memory-bounds regions areas can
be obtained in a coarser granularity in the interval between samples
timesteps. As for parallel applications running different flows may
have different behaviors which may vary according to the application
of load balancing, which causes the granularity of the samples is
thinner for better understanding its behavior. In addition to this approach, Laurenzano et
al.\cite{laurenzano2011reducing} present an approach finer granularity
for identifying the most appropriate processor frequency for each
application loop. Through multiple executions is set a model of
various sizes, it allows to find for example the most appropriate
setting for the given bond program, varying aspects that can influence
the energy reduction and performance improvement for the application.

There is also investigation to consider MPI or OpenMP parallel
applications. Freeh et al.\cite{freeh2005exploring} present an
approach to define the most suitable frequency for each phase of an
MPI application. Among the available frequencies, the approach looks
at what is the best frequency for a given node operate during the
execution of the application.  Another approach \cite{millani2016fr},
focused in OpenMP applications, analyzes the parallel regions of a
program using a rigorous evaluation using design of experiments and
screening designs.  According to their analysis based on seven
benchmarks, it is possible to reach a considerable gain in energy
reduction, eventually with no performance loss, depending on the
characteristics of the benchmark.  Their approach use manual
instrumentation to identify the code regions that are going to be
analzed. We focus instead in the automatic detection of such regions
based on hardware counters. In this paper we show our investigation in
how these counters can be measured along the application execution.

The next section describes our measurement and evaluation methodology
to collect hardware counters at a given frequency.

** Measurement and Evaluation Methodology
\label{sec:methodology}

The methodology used in the work first defines the compilation a
source code into binary. The program is run under the likwid-perfctr
tool that allows you to collect hardware counters for each processing
core. The data is processed by a script tailored to generate a
detailed application trace to carry out the data analysis. The Figure
\ref{figMetodologia} shows an overview of the methodology with all
such steps.

#+BEGIN_LaTeX
\begin{figure}[!htb] 
  \caption{Overview of the methodology.}
  \label{figMetodologia}
   \centering \includegraphics[width=\linewidth]{img/metodologiaWorkWsppd2016.pdf}
\end{figure}
#+END_LaTeX

We employ such methodology in three NAS Parallel applications: the 3D
Discrete Fast Fourier Transform (FT), the Lower-Upper Gauss-Seidel
Solver (LU) and the Conjugate Gradient (CG). The applications are
executed with 32 threads using the input size (class B) of the NAS
benchmark. The execution platform used was the beagle1, a Workstation
with 2 processors Intel (R) Xeon (R) E5-2650 CPU 2.00 GHz, each with 8
physical cores and Hyper-Threading technology.

The hardware counters are collected using likwid's timeline mode
\cite{treibig2010likwid}, configured to measure L2 and L3 cache miss
rate at a given time interval. Such interval between each metric
recording is defined according to the total execution time of each
application. For example, in the FT application, the measurement
period is defined as 30 milliseconds, generating about 172 samples
(for each of the 32 threads). For the LU application a 100
milliseconds is adopted, for 363 samples. For CG, a period of 50
milliseconds for 384 samples. According to the likwid authors,
adopting a period less than 100 milliseconds might generate non-valid
results. Even so, the global behavior is still valid if one aggregate
such information along time. We report FT and CG results under this
limitation in order to investigate how frequent measurements impact
our analysis of phase detection.

** Preliminary Results
\label{sec:results}

We present the L2 and L3 cache miss rate considering the aggregated
metrics for all cores of the two processors where we conducted
experiments. Points in the plots represented such aggregated values,
while lines are there only to show the metric trend along
time. Despite the fact that we aggregated values, we have looked to
each core cache level miss and they are all similar and homogeneous
(because of the regular nature of the applications we used),
justifying such aggregation to simplify the analysis.

*** Discrete 3D Fast Fourier Transform (NPB-FT, B Class) 

Figure \ref{figFT} depicts the L2 and L3 cache miss rate of the
Discrete 3D Fast Fourier Transform (NPB-FT, B Class) when measuring
metrics every 100 milliseconds. It clearly shows phases -- represented
by the peaks at regular intervals -- when taking into account the L2
cache miss rate. For the L3 cache miss we observe that after the
initialization phase (where a peak of 37% L3 miss rate is measured),
the rate decreases towards zero with minor outliers. The highest L2
cache miss rate found for FT is about 30% between 7.5 to 10 seconds
late time execution, while generally we can see that the observed
phases reach a level of 30% in L2 misses. The lowest L2 miss rate is
measured at 10% during the initialization phase.

#+BEGIN_LaTeX
\begin{figure}[!htb]
\includegraphics[width=\linewidth]{img/ft_L2_L3_100ms.pdf}
\caption{The L2 and L3 cache misses rate of the Discrete 3D Fast Fourier Transform (NPB-FT, B Class) when measuring metrics every 100 milliseconds.}
\label{figFT}
\end{figure}
#+END_LaTeX

*** Lower-Upper Gauss-Seidel Solver (NPB-LU, B Class)

Figure \ref{figLU} shows the L2 and L3 cache miss rate of the
Lower-Upper Gauss-Seidel Solver (NPB-LU, B Class). Behavior is clearly
different from the FT application, seen in Figure \ref{figFT}. We
observe that the L2 cache miss rate fluctuates around 20%, while the
L3 cache miss rate is about zero the whole execution, except during
the initialization phase, where a 13% rate is observed.

#+BEGIN_LaTeX
\begin{figure}[!htb]
\includegraphics[width=\linewidth,height=5cm]{img/lu_L2_L3_100ms.pdf}
\caption{The L2 and L3 cache misses rate of the Lower-Upper Gauss-Seidel Solver (NAS-LU, B Class) when measuring metrics every 100 milliseconds.}
\label{figLU}
\end{figure}
#+END_LaTeX

*** Conjugate Gradient (NPB-CG, B Class)

Figure \ref{figCG} shows the L2 and L3 caches miss rate for Conjugate
Gradient (NPB-CG, B Class) application. After the initialization
phase, behavior of both metrics becomes stable at about 38% of misses
for L2, and around zero for L3. Comparing against the previous
applications, we see that the L2 miss rate for CG is greater than the
others, suggesting that it is more memory-bound. This could
potentially lead to a more gains in energy reduction if a proper
processor frequency is selected.

#+BEGIN_LaTeX
\begin{figure}[!htb]
\includegraphics[width=\linewidth,height=5cm]{img/cg_L2_L3_100ms.pdf}
\caption{The L2 and L3 cache misses rate of the Conjugate Gradient (NPB-CG, B Class) when measuring metrics every 50 milliseconds.}
\label{figCG}
\end{figure}
#+END_LaTeX

** Conclusion
\label{sec:conclusion}

#+BEGIN_LaTeX
%\review{Reescrever do zero: começar lembrando a motivação, dizer
%rapidamente o objetivo e depois listar quais foram as contribuições
%deste trabalho. Lembrar que tu estás apenas medindo, não está
%detectando automaticamente as regiões pois isso envolveria aplicar um
%algoritmo nos dados que dissesse do timestamp X até o timestamp Y a
%aplicação se torna memory-bound. Terminas a conclusão com trabalhos
%futuros, onde teu objetivo será olhar outros benchmarks, e tentar
%propor uma forma de, baseado nas métricas coletadas, dizer quais são
%as regiões memory-bound.}
#+END_LaTeX

Parallel applications have many code regions, each one with unique
performance characteristics. If one is capable to detect which regions
are memory-bound, we might be able to reduce the processor frequency
with DVFS-based techniques without affecting too much the execution
time. The main goal of this study is to identify memory-bound code
regions.  The main contribution of this work is its methodology, which
defines the steps and tools necessary that should be used to identify
memory-bound portions of a parallel application. By applying such
methodology, we have been able to measure L2 and L3 cache misses for
three NPB applications, each one with different behavior.  As future
work, we plan to automatically identify those memory-bound regions
based on the hardware counters we have measured so far.

#+LATEX: \section*{Acknowledgements}
The results reported in this study were generated in virtue of the
agreement between Hewlett Packard Enterprise (HPE) and the Federal
University of Rio Grande do Sul (UFRGS), financed by resources in
return for the exemption or reduction of the IPI tax, granted by
Brazilian Law nº 8248, 1991, and its subsequent updates.

** References                                                        :ignore:

# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}

* TODO Bib file is here                                            :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib

@inproceedings{freeh2005exploring,
  title={Exploring the energy-time tradeoff in mpi programs on a power-scalable cluster},
  author={Freeh, Vincent W and Pan, Feng and Kappiah, Nandini and Lowenthal, David K and Springer, Robert},
  booktitle={19th IEEE International Parallel and Distributed Processing Symposium},
  pages={4a--4a},
  year={2005},
  organization={IEEE}
}

@inproceedings{laurenzano2011reducing,
  title={Reducing energy usage with memory and computation-aware dynamic frequency scaling},
  author={Laurenzano, Michael A and Meswani, Mitesh and Carrington, Laura and Snavely, Allan and Tikir, Mustafa M and Poole, Stephen},
  booktitle={European Conference on Parallel Processing},
  pages={79--90},
  year={2011},
  organization={Springer}
}

@inproceedings{spiliopoulos2012power,
  title={Power-Sleuth: A Tool for Investigating Your Program's Power Behavior},
  author={Spiliopoulos, Vasileios and Sembrant, Andreas and Kaxiras, Stefanos},
  booktitle={2012 IEEE 20th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems},
  pages={241--250},
  year={2012},
  organization={IEEE}
}

@incollection{schnorr2013visualizing,
  title={Visualizing More Performance Data Than What Fits on Your Screen},
  author={Schnorr, Lucas M and Legrand, Arnaud},
  booktitle={Tools for High Performance Computing 2012},
  pages={149--162},
  year={2013},
  publisher={Springer}
}

@inproceedings{millani2016fr,
author = {Millani, Luis Felipe and Schnorr, Lucas Mello},
title={Computation-Aware Dynamic Frequency Scaling: Parsimonious Evaluation of the Time-Energy Trade-off Using Design of Experiments},
year={2016},
booktitle={3rd International Workshop on Reproducibility in Parallel Computing (REPPAR)}
}

@book{jain1991art,
  title={Art of Computer Systems Performance Analysis: Techniques For Experimental Design Measurements Simulation and Modeling},
  author={Jain, R.},
  isbn={9781118858424},
  year={1991},
  publisher={Wiley}
}

@inproceedings{treibig2010likwid,
  title={Likwid: A lightweight performance-oriented tool suite for x86 multicore environments},
  author={Treibig, Jan and Hager, Georg and Wellein, Gerhard},
  booktitle={2010 39th International Conference on Parallel Processing Workshops},
  pages={207--216},
  year={2010},
  organization={IEEE}
}

#+end_src
* 2016-08-20 FT (new plots)                                        :noexport:

#+begin_src R :results output graphics :file img/ft_L2_L3_30ms.pdf :exports both :width 6 :height 3 :session
library(dplyr);
df2 <- read.csv("../../dados/exp1_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
df2 <- df2[df2$Metric == "M7", ];
df2$Metric <- "L2";
df3 <- read.csv("../../dados/exp2_NASandLikwid/ftB.csv", sep=" ", strip.white=T);
df3 <- df3[df3$Metric == "M7", ];
df3$Metric <- "L3";
df <- rbind (df2, df3);
df$Application <- "FT";
g <- df %>% group_by(Time,Metric,Application) %>% summarize (N=n(), mean=mean(Value)*100) %>% as.data.frame();

library(ggplot2);
ggplot(g, aes(x=Time, y=mean,color=as.factor(Metric))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,50) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="Cache Level") + facet_wrap(~Application) +
      labs(x = "Runtime (seconds)", y= "Average Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/ft_L2_L3_30ms.pdf]]

#+begin_src R :results output graphics :file img/ft_L2_L3_100ms.pdf :exports both :width 6 :height 3 :session
library(dplyr);
df2 <- read.csv("../../dados/exp3_NASandLikwid/ftl2.csv", sep=" ", strip.white=T);
df2 <- df2[df2$Metric == "M7", ];
df2$Metric <- "L2";
df3 <- read.csv("../../dados/exp3_NASandLikwid/ftl3.csv", sep=" ", strip.white=T);
df3 <- df3[df3$Metric == "M7", ];
df3$Metric <- "L3";
df <- rbind (df2, df3);
df$Application <- "FT";
g <- df %>% group_by(Time,Metric,Application) %>% summarize (N=n(), mean=mean(Value)*100) %>% as.data.frame();

library(ggplot2);
ggplot(g, aes(x=Time, y=mean,color=as.factor(Metric))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,50) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
      scale_color_discrete(name="Cache Level") + facet_wrap(~Application) +
      labs(x = "Runtime (seconds)", y= "Average Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/ft_L2_L3_100ms.pdf]]

* 2016-08-20 LU (new plots)                                        :noexport:

#+begin_src R :results output graphics :file img/lu_L2_L3_100ms.pdf :exports both :width 6 :height 3 :session
library(dplyr);
df2 <- read.csv("../../dados/exp1_NASandLikwid/luB.csv", sep=" ", strip.white=T);
df2 <- df2[df2$Metric == "M7", ];
df2$Metric <- "L2";
df3 <- read.csv("../../dados/exp2_NASandLikwid/luB.csv", sep=" ", strip.white=T);
df3 <- df3[df3$Metric == "M7", ];
df3$Metric <- "L3";
df <- rbind (df2, df3);
df$Application <- "LU";
g <- df %>% group_by(Time,Metric,Application) %>% summarize (N=n(), mean=mean(Value)*100) %>% as.data.frame();

library(ggplot2);
ggplot(g, aes(x=Time, y=mean,color=as.factor(Metric))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,50) +  
     theme(legend.position=c(0.9,0.8),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="Cache Level") + facet_wrap(~Application) +
      labs(x = "Runtime (seconds)", y= "Average Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/lu_L2_L3_100ms.pdf]]

* 2016-08-20 CG (new plots)                                        :noexport:

#+begin_src R :results output graphics :file img/cg_L2_L3_100ms.pdf :exports both :width 6 :height 3 :session
library(dplyr);
df2 <- read.csv("../../dados/exp1_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
df2 <- df2[df2$Metric == "M7", ];
df2$Metric <- "L2";
df3 <- read.csv("../../dados/exp2_NASandLikwid/cgB.csv", sep=" ", strip.white=T);
df3 <- df3[df3$Metric == "M7", ];
df3$Metric <- "L3";
df <- rbind (df2, df3);
df$Application <- "CG";
g <- df %>% group_by(Time,Metric,Application) %>% summarize (N=n(), mean=mean(Value)*100) %>% as.data.frame();

library(ggplot2);
ggplot(g, aes(x=Time, y=mean,color=as.factor(Metric))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,50) +  
     theme(legend.position=c(0.9,0.4),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="Cache Level") + facet_wrap(~Application) +
      labs(x = "Runtime (seconds)", y= "Average Cache Misses (%)");

#+end_src

#+RESULTS:
[[file:img/cg_L2_L3_100ms.pdf]]

* [18:00:44; 20.08.2016] Graph500 (new plots)                      :noexport:

- Script que o professor Lucas fez para mostrar o graph500:

#+begin_src R :results output :session :exports both
library(dplyr);
df2 <- read.csv("../../dados/exp1_graph500/graph500_L2.csv", sep=" ", strip.white=T);
df2 <- df2[df2$Metric == "M7", ];
df2$Metric <- "L2";
df3 <- read.csv("../../dados/exp1_graph500/graph500_L3.csv", sep=" ", strip.white=T);
df3 <- df3[df3$Metric == "M7", ];
df3$Metric <- "L3";
df <- rbind (df2, df3);
df$Application <- "Graph500";
g <- df %>% group_by(Time,Metric,Application) %>% summarize (N=n(), mean=mean(Value)*100) %>% as.data.frame();
head(g);
#+end_src

#+RESULTS:
:         Time Metric Application  N       mean
: 1 0.05056907     L3    Graph500 32  4.7365908
: 2 0.05057705     L2    Graph500 32  8.8282538
: 3 0.11702327     L2    Graph500 32 11.3548215
: 4 0.11772690     L3    Graph500 32  0.2876260
: 5 0.18278533     L2    Graph500 32 13.6554586
: 6 0.18569630     L3    Graph500 32  0.1767422

#+begin_src R :results output graphics :file img/exp1_graph500.pdf :exports both :session
library(ggplot2);
x = 120
ggplot(g[g$Time < x & g$Time > x-25,], aes(x=Time, y=mean,color=as.factor(Metric))) +
  	geom_line(size=0.5) + geom_point(size=1) + theme_bw() + ylim(0,50) +  
     theme(legend.position=c(0.9,0.5),
               legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
     scale_color_discrete(name="Cache Level") + facet_grid(Metric~Application) +
      labs(x = "Runtime (seconds)", y= "Average Cache Misses (%)");
#+end_src

#+RESULTS:
[[file:img/exp1_graph500.pdf]]



#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp1_graph500/graph500_L2.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:       Time Metric Socket  N     mean         se
: 1 104.5085     M7      1 16 40.60292 0.06767261
:       Time Metric Socket  N       mean           se
: 1 28.64286     M7      2 16 0.02027027 0.0006081081


#+begin_src R :results output :session *R* :exports both
library(dplyr);
df <- read.csv("../../dados/exp1_graph500/graph500_L3.csv", sep=" ", strip.white=T);
k <-    filter(df, df$Metric=='M7') %>% as.data.frame();

k$Socket <- ifelse(k$Core %% 2 == 0,1,2);

g <- k %>% group_by(Time,Metric,Socket) %>% summarize (N=n(), mean=mean(Value)*100, se=3*sd(Value)/sqrt(N)) %>% as.data.frame();

#identificando o maior valor
maxG <- max(g$mean);
g1_g <- filter(g,mean==maxG);
g1_g

#identificando o menor valor
minG <- min(g$mean);
g2_g <- filter(g,mean==minG);
g2_g

#+end_src

#+RESULTS:
:       Time Metric Socket  N     mean        se
: 1 53.78313     M7      1 16 64.11706 0.1781744
:       Time Metric Socket  N mean se
: 1  8.61918     M7      1 16    0  0
: 2 10.66638     M7      1 16    0  0
: 3 10.87421     M7      1 16    0  0
: 4 19.06684     M7      1 16    0  0
: 5 19.27063     M7      1 16    0  0
